<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Web Streams</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Francois+One&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h1>
            <span class="r-fit-text">Web Streams</span>
            <br />
            <span class="r-fit-text">From source to sink</span>
          </h1>
          <h3>Mattias Buelens</h3>
        </section>
        <section>
          <h1 class="heading-with-photo">
            <img src="assets/mattias.jpg" alt="Photo of Mattias" />
            Hi, I'm Mattias!
          </h1>
          <ul>
            <li>Lead software architect on THEOplayer</li>
            <li>
              Co-editor on the
              <a href="https://streams.spec.whatwg.org/">WHATWG Streams</a>
              standard
            </li>
            <li>
              Maintainer of
              <a href="https://www.npmjs.com/package/web-streams-polyfill"
                >web-streams-polyfill</a
              >
            </li>
          </ul>
        </section>
        <section>
          <section>
            <h1>What's the Streams API?</h1>
          </section>
          <section>
            <blockquote>
              <p>
                &laquo; The Streams API allows JavaScript to programmatically
                access streams of data received over the network and process
                them as desired by the developer. &raquo;
              </p>
              <cite
                ><a
                  href="https://developer.mozilla.org/en-US/docs/Web/API/Streams_API"
                  >Streams API, MDN</a
                ></cite
              >
            </blockquote>
          </section>
          <section>
            <h1>What's a stream?</h1>
          </section>
          <section data-auto-animate>
            <h2>A stream is an...</h2>
            <ul class="stretch">
              <li>
                ordered
                <ul class="fragment current-appear">
                  <li>process <em>N</em>th item before <em>N+1</em>th item</li>
                </ul>
              </li>
              <li>
                possibly asynchronous
                <ul class="fragment current-appear">
                  <li>next item might not be available immediately</li>
                  <li>
                    need to <code class="language-js">await</code> a
                    <code class="language-ts">Promise</code>
                  </li>
                </ul>
              </li>
              <li>
                data flow whose chunks
                <ul class="fragment current-appear">
                  <li>chunk = single bit of data</li>
                  <li>
                    can be anything: <code class="language-ts">string</code>,
                    <code class="language-ts">number</code>,&hellip;
                  </li>
                  <li>
                    byte streams: chunk =
                    <code class="language-ts">Uint8Array</code>
                  </li>
                </ul>
              </li>
              <li>
                can be read, processed or written
                <ul class="fragment current-appear">
                  <li>
                    different kinds of streams for producers, transformers and
                    consumers
                  </li>
                </ul>
              </li>
              <li>
                in an incremental fashion
                <ul class="fragment current-appear">
                  <li>can start processing before all data is available</li>
                  <li>don't have to buffer all data into memory</li>
                </ul>
              </li>
            </ul>
          </section>
          <section>
            <h2>Why streams?</h2>
            <ul>
              <li>
                process large amounts of data that can't fit in RAM
                <ul>
                  <li>e.g. full-text search through all of Wikipedia</li>
                  <li>
                    e.g. process uncompressed full HD video<br />
                    1 byte/channel &times; 3 channels (RGB) &times;
                    (1920&times;1080) pixels &times; 25 frames/second &times; 60
                    seconds = <strong>9.3 GB per minute</strong>
                  </li>
                </ul>
              </li>
              <li>
                serve content before the page is fully loaded
                <ul>
                  <li>
                    e.g.
                    <code class="language-js">renderToReadableStream()</code>
                    in React
                  </li>
                  <li>
                    e.g. start playing as soon as first few video frames have
                    been downloaded
                  </li>
                </ul>
              </li>
            </ul>
          </section>
        </section>
        <section>
          <section><h1>Readable streams</h1></section>
          <section>
            <h2><code class="language-js">ReadableStream</code></h2>
            <ul>
              <li>
                a <em>source</em> of data from which you can
                <em>read</em> chunks
              </li>
              <li>
                can be read by a single consumer at a time, using a "reader"
              </li>
            </ul>
            <pre><code class="language-js" data-trim data-line-numbers data-noescape>
              const readable = new ReadableStream(/* ignore for now */);
              const reader = reader.getReader();
              while (true) {
                // get the next chunk, waiting for it if necessary
                const { done, value } = await reader.read();
                if (done) {
                  // no more chunks, end of stream
                  break;
                }
                // got a chunk!
                console.log(`received chunk: ${value}`);
              }
            </code></pre>
          </section>
          <section>
            <h2>Example: streaming fetch</h2>
            <pre><code class="language-js" data-trim data-line-numbers data-noescape>
              // reading HTTP response without streaming
              const response = await fetch("https://example.com");
              const bytes = await response.arrayBuffer();
              console.log(`received ${bytes.byteLength} total bytes`);

              // reading HTTP response with streaming
              const response = await fetch("https://example.com");
              const body = response.body;       // ReadableStream&lt;Uint8Array&gt;
              const reader = body.getReader();  // ReadableStreamDefaultReader
              while (true) {
                const { done, value } = await reader.read();
                if (done) break; // reached end of stream
                console.log(`received chunk with ${value.byteLength} bytes`);
              }
            </code></pre>
          </section>
          <section data-auto-animate>
            <h2>Creating a <code class="language-js">ReadableStream</code></h2>
            <ul>
              <li>
                not all streams have to come from
                <code class="language-js">fetch()</code>
              </li>
              <li>make your own!</li>
            </ul>
            <pre
              data-id="code"
            ><code class="language-js" data-trim data-line-numbers data-noescape>
              const readable = new ReadableStream({
                start(controller) {
                  // called immediately during construction
                  // for setup: open connections, add listeners,...
                },
                pull(controller) {
                  // called when more data is needed
                },
                cancel(reason) {
                  // called when consumer is no longer interested
                  // for cleanup: close connection, remove listeners,...
                }
              });
            </code></pre>
          </section>
          <section data-auto-animate>
            <h2>Creating a <code class="language-js">ReadableStream</code></h2>
            <ul>
              <li>use the given controller to update the stream</li>
            </ul>
            <pre
              data-id="code"
            ><code class="language-js" data-trim data-line-numbers data-noescape>
              const readable = new ReadableStream({
                pull(controller) {
                  // to push a chunk onto the stream:
                  controller.enqueue("some data");
                  // to close the stream normally (i.e. reached end of file)
                  controller.close();
                  // to close the stream abnormally (i.e. I/O error)
                  controller.error(new Error("oh no"));
                }
              });
            </code></pre>
          </section>
          <section>
            <h2>Example: a stream of events</h2>
            <pre><code class="language-js" data-trim data-line-numbers data-noescape>
              let clickListener;
              const clickStream = new ReadableStream({
                start(controller) {
                  // add listener which pushes clicks onto the stream
                  clickListener = (event) =>
                    controller.enqueue({ x: event.pageX, y: event.pageY });
                  window.addEventListener('click', clickListener);
                },
                pull(controller) {
                  // nothing to do here, we can't "request" more click events
                },
                cancel(reason) {
                  window.removeEventListener('click', clickListener);
                }
              });
            </code></pre>
            <p>= a "push source": data is added even when no-one is reading</p>
          </section>
          <section>
            <h2>Example: an infinite stream of blog posts</h2>
            <pre><code class="language-js" data-trim data-line-numbers data-noescape>
              let page = 1;
              const blogPostStream = new ReadableStream({
                async pull(controller) {
              //^^^^^ stream will wait for previous pull
              //      before starting a new pull
                  const response = await fetch(`/api/posts?page=${page}`);
                  const posts = await response.json();
                  posts.forEach((post) => controller.enqueue(post));
                  page += 1;
                }
              });
            </code></pre>
            <p>= a "pull source": data is only added when requested</p>
          </section>
          <section data-auto-animate>
            <h2>Problem: stream can only be read once</h2>
            <pre
              data-id="code"
            ><code class="language-js" data-trim data-line-numbers data-noescape>
              const response = await fetch("https://example.com");
              const body = response.body;
              const reader1 = body.getReader();
              while (true) {
                const { done, value } = await reader1.read();
                if (done) break; // reached end of stream
                console.log(`received chunk with ${value.byteLength} bytes`);
              }

              const reader2 = body.getReader();
              while (true) {
                const { done, value } = await reader2.read();
                // ^ will always be { done: true, value: undefined };
                if (done) break;
              }
            </code></pre>
          </section>
          <section data-auto-animate>
            <h2>Solution: <code class="language-js">tee()</code></h2>
            <pre
              data-id="code"
            ><code class="language-js" data-trim data-line-numbers data-noescape>
              const response = await fetch("https://example.com");
              const body = response.body;
              const [body1, body2] = body.tee();
              const reader1 = body1.getReader();
              while (true) {
                const { done, value } = await reader1.read();
                if (done) break; // reached end of stream
                console.log(`received chunk with ${value.byteLength} bytes`);
              }

              const reader2 = body2.getReader();
              while (true) {
                const { done, value } = await reader2.read();
                // ^ receives the same chunks as reader1
                if (done) break;
              }
            </code></pre>
          </section>
        </section>
        <section>
          <section>
            <h1>Writable streams</h1>
          </section>
          <section>
            <h2><code class="language-js">WritableStream</code></h2>
            <ul>
              <li>
                a <em>destination</em> for data into which you can
                <em>write</em> chunks
              </li>
              <li>e.g.: writing to a file using the File System API</li>
            </ul>
            <pre><code class="language-js" data-trim data-line-numbers data-noescape>
              // example from FileSystemWritableFileStream on MDN
              // open a "Save file" dialog to let the user choose a file
              const newFileHandle = await window.showSaveFilePicker();
              // create a WritableStream from our file
              const writable = await newFileHandle.createWritable();
              // write to the file
              const writer = writable.getWriter();
              await writer.write("This is my file content");
              // close the file and write the contents to disk
              await writer.close();
            </code></pre>
          </section>
          <section data-auto-animate>
            <h2>Creating a <code class="language-js">WritableStream</code></h2>
            <pre
              data-id="code"
            ><code class="language-js" data-trim data-line-numbers data-noescape>
              const writable = new WritableStream({
                start(controller) {
                  // called immediately during construction
                },
                write(chunk, controller) {
                  // called when a new chunk is written
                },
                close() {
                  // called when no more data will be written
                  // (e.g. end of input)
                },
                abort(reason) {
                  // called when producer encounters a problem
                }
              });
            </code></pre>
          </section>
          <section>
            <h2>Example: a simple logger</h2>
            <pre><code class="language-js" data-trim data-line-numbers data-noescape>
              const logStream = new WritableStream({
                write(chunk) {
                  console.log("Wrote chunk: ", chunk);
                },
                close() {
                  console.log("Done writing");
                },
                abort(reason) {
                  console.error("Aborted by producer: ", reason);
                }
              });
            </code></pre>
          </section>
          <section>
            <h2>Piping a readable to a writable stream</h2>
            <pre
              data-id="code"
            ><code class="language-js" data-trim data-line-numbers data-noescape>
              const readable = new ReadableStream(/* ... */);
              const writable = new WritableStream(/* ... */);
              await readable.pipeTo(writable);
            </code></pre>
            <ul>
              <li>
                reads every chunk from
                <code class="language-js">readable</code> and writes it to
                <code class="language-js">writable</code>
              </li>
              <li>closes writable once readable closes</li>
              <li>propagates errors</li>
            </ul>
          </section>
        </section>
        <section>
          <section>
            <h1>Transform streams</h1>
          </section>
          <section>
            <h2><code class="language-js">TransformStream</code></h2>
            <ul>
              <li>a pair of writable and readable streams</li>
              <li>
                data written into the writable end is transformed and made
                available through the readable end
              </li>
            </ul>
            <pre><code class="language-js" data-trim data-line-numbers data-noescape>
              const upperCaseTransform = new TransformStream({
                start(controller) {},
                transform(chunk, controller) {
                  // called when a new chunk is written to the writable end
                  // can transform it and enqueue to the readable end:
                  controller.enqueue(chunk.toUpperCase());
                },
                flush(controller) {}, // called when writable end is closed
                cancel(reason) {
                  // called when writable end is aborted
                  // or readable end is cancelled
                }
              });
            </code></pre>
          </section>
          <section data-auto-animate>
            <h2>Using a transform stream</h2>
            <pre
              data-id="code"
            ><code class="language-js" data-trim data-line-numbers data-noescape>
              const input = new ReadableStream(/* ... */);
              const upperCaseTransform = new TransformStream({
                transform(chunk, controller) {
                  controller.enqueue(chunk.toUpperCase());
                }
              });
              const output = new WritableStream(/* ... */);

              input
                .pipeTo(upperCaseTransform.writable)
                .catch(() => {});
              // ^^^ important: don't await the pipeTo() call!
              upperCaseTransform.readable
                .pipeTo(output).catch(console.error);
            </code></pre>
          </section>
          <section data-auto-animate>
            <h2>Using a transform stream</h2>
            <pre
              data-id="code"
            ><code class="language-js" data-trim data-line-numbers data-noescape>
              const input = new ReadableStream(/* ... */);
              const upperCaseTransform = new TransformStream({
                transform(chunk, controller) {
                  controller.enqueue(chunk.toUpperCase());
                }
              });
              const output = new WritableStream(/* ... */);

              input
                .pipeThrough(upperCaseTransform)
                .pipeTo(output).catch(console.error);
            </code></pre>
          </section>
          <section data-auto-animate>
            <h2>Pipe chains</h2>
            <pre
              data-id="code"
            ><code class="language-js" data-trim data-line-numbers data-noescape>
              const input = new ReadableStream(/* ... */);
              const upperCaseTransform = new TransformStream(/* ... */);
              const reverseTransform = new TransformStream({
                transform(chunk, controller) {
                  controller.enqueue(chunk.reverse());
                }
              });
              const output = new WritableStream(/* ... */);

              input
                .pipeThrough(upperCaseTransform)
                .pipeThrough(reverseTransform)
                .pipeTo(output).catch(console.error);
            </code></pre>
          </section>
        </section>
        <section>
          <section>
            <h1>Queuing and backpressure</h1>
          </section>
          <section>
            <h2>Internal queues</h2>
            <ul>
              <li>
                <code class="language-js">ReadableStream</code>: holds chunks
                enqueued by the backing source which the reader hasn't read yet
              </li>
              <li>
                <code class="language-js">WritableStream</code>: holds chunks
                written by the writer which the backing sink hasn't processed
                yet
              </li>
              <li>
                <code class="language-js">TransformStream</code>: has queues on
                writable&nbsp;end (pre&#8209;transform) and readable&nbsp;end
                (post&#8209;transform)
              </li>
            </ul>
          </section>
          <section>
            <h2>Filling up the queue</h2>
            <p>
              each stream has a queuing strategy, which determines the desired
              size of the stream's queue
            </p>
            <pre><code class="language-js" data-trim data-line-numbers data-noescape>
              let s1 = new CountQueuingStrategy({ highWaterMark: 10 });
              // ^ want at least 10 chunks in the queue
              let s2 = new ByteLengthQueuingStrategy({ highWaterMark: 1024 });
              // ^ want at least 1 KB in the queue (for byte streams)

              let readable = new ReadableStream({
                pull(controller) {
                  console.log("pull called");
                  controller.enqueue("chunk");
                }
              }, new CountQueuingStrategy({ highWaterMark: 10 }));
              // ^ calls pull() 10 times, then pauses
            </code></pre>
          </section>
          <section>
            <h2>Backpressure</h2>
            <ul>
              <li>
                once queue's size exceeds desired size, it will apply
                backpressure
                <ul>
                  <li>
                    <code class="language-js">ReadableStream</code>: stop
                    calling <code class="language-js">pull()</code>
                  </li>
                  <li>
                    <code class="language-js">WritableStream</code>: signal "not
                    ready" to writer
                  </li>
                </ul>
              </li>
              <li>
                in a long pipe chain, backpressure signal propagates from the
                back to the front to normalize the flow of chunks
                <ul>
                  <li>
                    when it reaches the original source, it should try to
                    produce chunks at a slower pace
                  </li>
                </ul>
              </li>
            </ul>
          </section>
        </section>
        <section>
          <section>
            <h1>New and upcoming features</h1>
          </section>
          <section data-auto-animate data-auto-animate-id="iterator">
            <h2>Async iteration</h2>
            <p>
              integrates with JavaScript's
              <code class="language-js">for await of</code>
            </p>
            <pre
              data-id="code"
            ><code class="language-js" data-trim data-line-numbers data-noescape>
              // before:
              const readable = new ReadableStream(/* ... */);
              const reader = reader.getReader();
              while (true) {
                const { done, value } = await reader.read();
                if (done) {
                  break;
                }
                console.log(`received chunk: ${value}`);
              }
            </code></pre>
          </section>
          <section data-auto-animate data-auto-animate-id="iterator">
            <h2>Async iteration</h2>
            <p>
              integrates with JavaScript's
              <code class="language-js">for await of</code>
            </p>
            <pre
              data-id="code"
            ><code class="language-js" data-trim data-line-numbers data-noescape>
              // after:
              const readable = new ReadableStream(/* ... */);
              for await (const value of readable) {
                console.log(`received chunk: ${value}`);
              }
            </code></pre>
          </section>
          <section>
            <h2>Async iteration</h2>
            <blockquote>
              <img
                src="assets/caniuse-asynciterator.png"
                alt="Browser support for ReadableStream API: @@asyncIterator. Chrome supports it starting from version 124 and Firefox starting from version 110. Safari doesn't support it yet."
              />
              <cite
                ><a
                  href="https://caniuse.com/mdn-api_readablestream_--asynciterator"
                  >caniuse.com</a
                ></cite
              >
            </blockquote>
          </section>
          <section>
            <h2>
              <code class="language-js">ReadableStream.from()</code>
            </h2>
            <p>
              converts arrays, iterables or async iterables to a
              <code class="language-js">ReadableStream</code>
            </p>
            <pre><code class="language-js" data-trim data-line-numbers data-noescape>
              ReadableStream.from(["an", "array", "of", "chunks"]);
              ReadableStream.from(new Set(["a", "set", "of", "chunks"]));
              ReadableStream.from((async function*() {
                yield "an";
                yield "async";
                yield "generator";
              })());

              // in Node.js:
              ReadableStream.from(fileHandle.createReadStream());
            </code></pre>
          </section>
          <section>
            <h2>
              <code class="language-js">ReadableStream.from()</code>
            </h2>
            <blockquote>
              <img
                src="assets/caniuse-from.png"
                alt="Browser support for ReadableStream API: from() static method. Firefox supports it starting from version 117. Chrome and Safari don't support it yet."
              />
              <cite
                ><a
                  href="https://caniuse.com/mdn-api_readablestream_from_static"
                  >caniuse.com</a
                ></cite
              >
            </blockquote>
          </section>
          <section>
            <h2>Streaming across threads</h2>
            <p>
              all streams are transferable, which means you can
              <code class="language-js">postMessage()</code> them to a worker
            </p>
            <pre><code class="language-js" data-trim data-line-numbers data-noescape>
              // page.js
              const worker = new Worker("worker.js");
              const { readable, writable } = new TransformStream();
              worker.postMessage(readable, { transfer: [readable] });
              const writer = writable.getWriter();
              writer.write("Hello world!"); // send to worker's ReadableStream
              writer.close();

              // worker.js
              onmessage = async (event) => {
                const readable = event.data; // transferred ReadableStream
                for await (const chunk of readable) {
                  console.log(chunk); // -> "Hello world!"
                }
              };
            </code></pre>
          </section>
          <section>
            <h2>Streaming across threads</h2>
            <p>move CPU-intensive work to another thread</p>
            <pre><code class="language-js" data-trim data-line-numbers data-noescape>
              // page.js
              const worker = new Worker("worker.js");
              worker.onmessage = async (event) => {
                const transform = event.data; // transferred TransformStream
                const input = (await fetch("https://example.com")).body;
                const output = input.pipeThrough(transform);
                for await (const chunk of output) { console.log(chunk); }
              };
              // worker.js
              const transform = new TransformStream({
                transform(chunk, controller) {
                  controller.enqueue(doExpensiveTransform(chunk));
                }
              });
              self.postMessage(transform, { transfer: [transform] });
            </code></pre>
          </section>
          <section>
            <h2>Streaming across threads</h2>
            <blockquote>
              <img
                src="assets/caniuse-transferable.png"
                alt="Browser support for ReadableStream API: transferable. Chrome supports it starting from version 87 and Firefox starting from version 103. Safari doesn't support it yet."
              />
              <cite
                ><a
                  href="https://caniuse.com/mdn-api_readablestream_transferable"
                  >caniuse.com</a
                ></cite
              >
            </blockquote>
          </section>
          <section>
            <h2>Can't wait? Polyfill it!</h2>
            <ul>
              <li>
                <a href="https://www.npmjs.com/package/web-streams-polyfill"
                  >web-streams-polyfill on npm</a
                >
                <ul>
                  <li>✅ Async iteration</li>
                  <li>
                    ✅ <code class="language-js">ReadableStream.from()</code>
                  </li>
                </ul>
              </li>
              <li>
                <a href="https://www.npmjs.com/package/remote-web-streams"
                  >remote-web-streams on npm</a
                >
                <ul>
                  <li>✅ Transferable streams (with different API)</li>
                </ul>
              </li>
            </ul>
          </section>
        </section>
      </div>
    </div>
    <script type="module" src="/src/main.ts"></script>
  </body>
</html>
